{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Applications - Mini Project 2\n",
    "> By Oliver Dietsche & Simon Peier\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We decided to use part of the [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet) dataset. \n",
    "\n",
    "According to the description in Kaggle, the Tiny ImageNet dataset has the following properties: \"Tiny ImageNet contains 200 classes for training. Each class has 500 images. The test set contains 10,000 images. All images are 64x64 colored ones.\" It fulfills all the required key characteristics, except that it has too many classes (and thus too many samples). To meet all criteria, we pre-processed the data-set and chose 6 classes to use. As the dataset contains lots of classes of different categories, we settled for classes from the animal realm.\n",
    "\n",
    "### Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"dataset\"\n",
    "words_file = os.path.join(data_dir, \"words.txt\")\n",
    "image_size = (64, 64)\n",
    "\n",
    "# Load words.txt\n",
    "with open(words_file, 'r') as f:\n",
    "    class_labels = {}\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')\n",
    "        class_labels[line[0]] = line[1]\n",
    "\n",
    "label_to_int_map = {label: i for i, label in enumerate(class_labels.values())}\n",
    "\n",
    "def label_to_int(label):\n",
    "    return label_to_int_map[label]\n",
    "\n",
    "def int_to_label(intVal):\n",
    "    return list(label_to_int_map.keys())[intVal]\n",
    "\n",
    "# Load images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for folder_name in os.listdir(data_dir):\n",
    "    if folder_name.startswith('n'):\n",
    "        label = class_labels[folder_name]\n",
    "        image_folder_path = os.path.join(data_dir, folder_name, \"images\")\n",
    "        for image_name in os.listdir(image_folder_path):\n",
    "            image_path = os.path.join(image_folder_path, image_name)\n",
    "            image = img_to_array(load_img(image_path))\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Range of pixel values: [{images.min()};{images.max()}]\")\n",
    "\n",
    "images = images / 255.0\n",
    "\n",
    "print(f\"Range of pixel values: [{images.min()};{images.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test and training data\n",
    "train_val_imgs, test_imgs, train_val_labels, test_labels = train_test_split(images, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation data\n",
    "train_imgs, val_imgs, train_labels, val_labels = train_test_split(train_val_imgs, train_val_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and print image stats\n",
    "(total_samples, image_height, image_width, num_channels) = images.shape\n",
    "test_samples = test_imgs.shape[0]\n",
    "train_samples = train_imgs.shape[0]\n",
    "val_samples = val_imgs.shape[0]\n",
    "print(f\"Total samples:       {total_samples}\")\n",
    "print(f\"Testing samples:     {test_samples}\")\n",
    "print(f\"Training samples:    {train_samples}\")\n",
    "print(f\"Validation samples:  {val_samples}\\n\")\n",
    "print(f\"Image height:        {image_height}\")\n",
    "print(f\"Image width:         {image_width}\")\n",
    "print(f\"Number of channels:  {num_channels}\")\n",
    "\n",
    "# Count samples per class\n",
    "samples_per_class = {}\n",
    "for label in labels:\n",
    "    samples_per_class[label] = samples_per_class.get(label, 0) + 1\n",
    "\n",
    "# Determine if dataset is balanced\n",
    "class_labels = list(samples_per_class.keys())\n",
    "class_counts = list(samples_per_class.values())\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.title('Samples per Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot sample image for every class\n",
    "num_samples_to_plot = 6\n",
    "classes_to_plot = np.random.choice(list(samples_per_class.keys()), num_samples_to_plot, replace=False)\n",
    "\n",
    "fig, axs = plt.subplots(1, num_samples_to_plot, figsize=(15, 3))\n",
    "\n",
    "for i, class_label in enumerate(classes_to_plot):\n",
    "    class_indices = np.where(labels == class_label)[0]\n",
    "    sample_index = np.random.choice(class_indices)\n",
    "    sample_image = images[sample_index]\n",
    "    axs[i].imshow(sample_image)\n",
    "    axs[i].set_title(class_label)\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting\n",
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_labels)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(8, 3, padding='same', activation='relu', strides=3, input_shape=(image_height, image_width, 3)),\n",
    "    layers.MaxPooling2D(3),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_labels_int = np.array([label_to_int(l) for l in train_labels])\n",
    "val_labels_int = np.array([label_to_int(l) for l in val_labels])\n",
    "\n",
    "history = model.fit(train_imgs, train_labels_int, epochs=100, \n",
    "                    validation_data=(val_imgs, val_labels_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 2])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Both the training and validation accuracy have a big increase in accuracy in the first few epochs but it quickly stagnates around 60% accuracy. This is an indicator that the model might not be able to capture the complexity our categorization within the defined layers.\n",
    "- The loss graph also performs very poorly, with training and validation separating over time instead of growing towards eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_imgs)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels = [int_to_label(l) for l in predicted_labels]\n",
    "\n",
    "labels = np.unique(val_labels)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "norm_options = [None, 'true', 'pred', 'all']\n",
    "\n",
    "for ax, norm in zip(axes, norm_options):\n",
    "    cm = confusion_matrix(val_labels, predicted_labels, normalize=norm)\n",
    "    \n",
    "    disp = ax.matshow(cm, cmap='Blues')\n",
    "    ax.set_title(f'Normalization: {str(norm)}')\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            text = ax.text(j, i, f'{cm[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "fig.colorbar(disp, ax=axes, orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add interpretation of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overfitting_model = models.Sequential([\n",
    "    layers.Conv2D(48, 4, activation='relu', padding='same', input_shape=(image_height, image_width, 3)),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Conv2D(64, 4, activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Conv2D(96, 4, activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Conv2D(128, 4, activation='relu', padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(48),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "overfitting_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfitting_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = overfitting_model.fit(train_imgs, train_labels_int, epochs=30, \n",
    "                    validation_data=(val_imgs, val_labels_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0,1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 5])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = overfitting_model.predict(val_imgs)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels = [int_to_label(l) for l in predicted_labels]\n",
    "\n",
    "labels = np.unique(val_labels)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "norm_options = [None, 'true', 'pred', 'all']\n",
    "\n",
    "for ax, norm in zip(axes, norm_options):\n",
    "    cm = confusion_matrix(val_labels, predicted_labels, normalize=norm)\n",
    "    \n",
    "    disp = ax.matshow(cm, cmap='Blues')\n",
    "    ax.set_title(f'Normalization: {str(norm)}')\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            text = ax.text(j, i, f'{cm[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "fig.colorbar(disp, ax=axes, orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Our model behaves as predicted. \n",
    "\n",
    "The training accuracy increases continuously, reaching very high values, as the model memorizes the training data and is able to classify it accurately. The validation accuracy on the other hand increases initially, but after a certain time it plateaus, because the model becomes too specialized to the training data.\n",
    "\n",
    "The training loss steadily decreases up until a certain point when it reaches very low values. This indicates, that the model is fitting the data extremely well. However the validation loss, after initially decreasing, begins to increase once the model starts overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing\n",
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimized_model = models.Sequential([\n",
    "    layers.Conv2D(48, 4, kernel_regularizer=l2(0.001), activation='relu', padding='same', input_shape=(image_height, image_width, 3)),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(64, 4, kernel_regularizer=l2(0.001), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(96, 4, kernel_regularizer=l2(0.001), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(128, 4, kernel_regularizer=l2(0.001), activation='relu', padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(48, kernel_regularizer=l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, kernel_regularizer=l2(0.001), activation='softmax'),\n",
    "])\n",
    "\n",
    "optimized_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = optimized_model.fit(train_imgs, train_labels_int, epochs=40, \n",
    "                    validation_data=(val_imgs, val_labels_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 5])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = optimized_model.predict(val_imgs)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels = [int_to_label(l) for l in predicted_labels]\n",
    "\n",
    "labels = np.unique(val_labels)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "norm_options = [None, 'true', 'pred', 'all']\n",
    "\n",
    "for ax, norm in zip(axes, norm_options):\n",
    "    cm = confusion_matrix(val_labels, predicted_labels, normalize=norm)\n",
    "    \n",
    "    disp = ax.matshow(cm, cmap='Blues')\n",
    "    ax.set_title(f'Normalization: {str(norm)}')\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            text = ax.text(j, i, f'{cm[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "fig.colorbar(disp, ax=axes, orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
